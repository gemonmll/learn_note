## **1结合代码理解 transformer模型**
> 相关资料
> ![Alt text](image-50.png)
> NLP的各种任务（NLU分词，NLG文本生成）
> 演进:rnn lstm transformer bert llm
> ![Alt text](image-51.png)
> transformer 整体结构
> ![Alt text](image-52.png)
> ![Alt text](image-53.png)
> ![Alt text](image-54.png)
> **BERT结构**
> 只保留了ENcoder结构
> 采用MLM训练。随机mask掉一些字，然后猜测字是什么
> 隐层训练好后，进行下游任务：句子分类，关系推理，信息抽取
> ![Alt text](image-55.png)
> **llm的诞生：gpt **
> 是使用了生成类任务
> 采用auto-regressice方式训练，预测下一个字是什么
> 只保留了Decoder层
> ![Alt text](image-56.png)
> **transformers**
> 每一家的基础框架
> huggingface提供的开源框架
> ![Alt text](image-57.png)
> ![Alt text](image-58.png)
> ## **程序实战部分**
> **tokinizer**
> 文本序列的入模
> ![Alt text](image-59.png)
> 字典大小，encode decode
> ![Alt text](image-60.png)
> 样例llm的讲解：glm
> ![Alt text](image-61.png)
> ![Alt text](image-62.png)
> 模型的加载
> ![Alt text](image-63.png)
> 一些模块的代码 block mlp
> ![Alt text](image-64.png)
> ![Alt text](image-65.png)
> ![Alt text](image-66.png)
> loss
> 困惑度，困惑度越高，模型越不置信
> ![Alt text](image-67.pn
> ![Alt text](image-68.png)
> 生成例子
> ![Alt text](image-69.png)
> ![Alt text](image-70.png)
> ![Alt text](image-71.png)
> openai
> ![Alt text](image-72.png)