## **03 框架选型和推理优化**
> ### **第一部分 大模型框架选择**
> **大模型框架选择**
> ![Alt text](image-60.png)
> ![Alt text](image-61.png)
> 各大框架的优缺点
> deepspeed+megatron 预训练更为常用
> accelerate更适用于微调
> ![Alt text](image-62.png)
> **数据并行 zero分片**
> dp\ddp\fsdp
> ![Alt text](image-63.png)
> **dp 数据并行**
> ![Alt text](image-64.png)
> **算子介绍**
> reduce-scatter\all-gather\all-reduce
> ![Alt text](image-65.png)
> **ddp 分布式数据并行**
> 没有ps,参数服务器
> ![Alt text](image-66.png)
> **zero阶段**
> zero 1 阶段 数据并行
> ![Alt text](image-67.png)
> 2阶段 分层更新梯度
> ![Alt text](image-68.png)
> 3阶段
> ![Alt text](image-69.png)
> **accelerate使用**
> 增加一行，使用accelerator的并行
> 重新包装下
> ![Alt text](image-70.png)
> 运行
> ![Alt text](image-71.png)
> **各个模型框架使用场景**
> ![Alt text](image-72.png)
> ## **第二部分模型推理优化**
> ![Alt text](image-73.png)
> 模型推理压缩优化
> ![Alt text](image-74.png)
> **知识蒸馏**
> ![Alt text](image-75.png)
> ![Alt text](image-76.png)
> 让两个模型最后一层输出尽量相似，kl散度和l2距离
> ![Alt text](image-77.png)
> 知识蒸馏做法
> ![Alt text](image-78.png)
> 多个中间层学习
> ![Alt text](image-79.png)
> ![Alt text](image-80.png)
> **模型剪枝**
> 去除模型参数矩阵中的一部分
> ![Alt text](image-81.png)
> 30%左右的参数都可以舍弃
> ![Alt text](image-82.png)
> head剪枝
> ![Alt text](image-83.png)
> layer剪枝
> ![Alt text](image-84.png)
> **模型量化**
> 浮点计算量化
> ![Alt text](image-85.png)
> **参数共享**
> 每一层使用相同的参数
> ![Alt text](image-87.png)
> **低秩分解**
> ![Alt text](image-88.png)
> **结构搜索**
> ![Alt text](image-89.png)
> **不同推理优化技术对比**
> ![Alt text](image-90.png)
> **kv cache decoder优化的一种方法**
> ![Alt text](image-91.png)