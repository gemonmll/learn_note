## **目录**
> ![Alt text](image-22.png)
## **第一部分 模型训练**
> ![Alt text](image-23.png)
> ### **1.1 trainer**
> trainer arguments 封装成一个类
> ![Alt text](image-24.png)
> 简单用法
> ![Alt text](image-25.png)
> training auguments 拆成多个类分别设置参数
> ![Alt text](image-26.png)
> ![Alt text](image-27.png)
> ### **1.2 input processing基本流程**
> promt（已经包含了上下文，不计算损失函数）和completion 序列化 转化成dataset
> dataset（注意长度，promt前面截断） padding mask ，转为batch化
> ![Alt text](image-28.png)
> 示例
> ![Alt text](image-30.png)
> ![Alt text](image-29.png)
> ### **1.3 evaluate and save**
> 评估模型当前性能
> 一般用ppl困惑度，token预测精度衡量
> ![Alt text](image-31.png)
> 示例
> ![Alt text](image-32.png)
> ## **2 训练超参介绍**
> ![Alt text](image-33.png)
> **2.1 steps概览**
> ![Alt text](image-34.png)
> **2.2 learningRate**
> ![Alt text](image-35.png)
> **2.3 zerostage概览**
> 不同的阶段节省显存
> ![Alt text](image-36.png)
> ## **3 优化器介绍**
> 高效调节模型参数
> ![Alt text](image-37.png)
> ![Alt text](image-38.png)
> adam和adamw概览 使用权重衰减来防止过拟合
> ![Alt text](image-39.png)
> ![Alt text](image-40.png)
> group parameters 
> ![Alt text](image-41.png)
> ## **第二部分 训练优化**
> ![Alt text](image-42.png)
> **1 精度优化**
> 低精度表示高精度
> ![Alt text](image-43.png)
> bf16，google提出的一个小数表示方法
> ![Alt text](image-44.png)
> 整数表示
> ![Alt text](image-45.png)
> ![Alt text](image-46.png)
> 混合精度
> ![Alt text](image-47.png)
> **2 显存优化**
> **gradient checkpoint**
> 定点保存前向结果，并非全部保存
> ![Alt text](image-48.png)
> ![Alt text](image-50.png)
> **zero stage 回顾**
> 不同阶段优化显存方案
> 显存需求降低，速度变慢
> ![Alt text](image-51.png)
> zero stage具体意思
> ![Alt text](image-56.png)
> **offload**
> 将梯度和优化器状态卸载到cpu上，在cpu上更新参数
> ![Alt text](image-52.png)
> ![Alt text](image-53.png)
> **其他优化**
> ![Alt text](image-54.png)
> **3 并行优化**
> **DP 并行优化**
> data parallelism 数据并行
> 借助之前介绍的zerostage和cpuoffload
> 多卡跑多个batch
> ![Alt text](image-55.png)
> **pp pipeline 并行优化**
> 把模型不同部分放到不同卡上
> ![Alt text](image-57.png)
> **TP tensor 模型并行**
> 张量并行
> ![Alt text](image-58.png)
> **3d并行**
> ![Alt text](image-59.png)